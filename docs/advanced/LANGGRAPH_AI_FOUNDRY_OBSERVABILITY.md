# LangGraph + Azure AI Foundry Observability Integration

**Status**: ✅ Native Integration Available (2025)

## Overview

Azure AI Foundry provides **native, unified observability** for LangGraph agents through OpenTelemetry-compliant tracing. No custom instrumentation needed - just install the Azure integration package.

## Key Benefits

✅ **Automatic Tracing**
- LangGraph graph execution
- Agent invocations
- Tool calls
- LLM requests
- State transitions

✅ **Unified Observability**
- Works across Microsoft Agent Framework, Semantic Kernel, LangChain, LangGraph, OpenAI Agents SDK
- Single pane of glass for all agent frameworks
- Standardized telemetry via OpenTelemetry

✅ **Production Ready**
- Built-in integration with Application Insights
- Real-time monitoring dashboards
- Performance analytics
- Error tracking

## How It Works

### 1. Installation

```bash
pip install langchain-azure-ai langgraph langchain-openai azure-identity
```

**Note**: Integration currently available in **Python only** (as of 2025)

### 2. Configuration

**Environment variables:**
```bash
# Azure AI Foundry
AZURE_AI_PROJECT_NAME=travel-planning
AZURE_SUBSCRIPTION_ID=xxx
AZURE_RESOURCE_GROUP=rg-ai-foundry

# Application Insights (automatic with AI Foundry)
APPLICATIONINSIGHTS_CONNECTION_STRING=InstrumentationKey=xxx

# OpenAI endpoint
AZURE_OPENAI_ENDPOINT=https://xxx.openai.azure.com/
AZURE_OPENAI_API_KEY=xxx
```

### 3. Enable Tracing in Your LangGraph App

**Option A: Automatic (Recommended)**

```python
# Just import langchain-azure-ai and it auto-instruments
from langchain_azure_ai import init_tracing

# Initialize tracing (one-time, at app startup)
init_tracing(
    project_name="travel-planning",
    export_to="application_insights"
)

# Your existing LangGraph code - no changes needed!
from langgraph.graph import StateGraph

graph = StateGraph(TravelPlanningState)
graph.add_node("supervisor", supervisor_node)
graph.add_node("flight_agent", flight_agent_node)
# ... rest of your graph

# When you run the graph, traces automatically flow to AI Foundry
result = await graph.ainvoke(state)
```

**Option B: Manual (More Control)**

```python
from azure.ai.resources import AIResourcesClient
from azure.identity import DefaultAzureCredential
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from azure.monitor.opentelemetry.exporter import AzureMonitorTraceExporter

# Set up Azure AI Foundry client
credential = DefaultAzureCredential()
ai_client = AIResourcesClient(
    endpoint="https://xxx.api.azureml.ms",
    credential=credential
)

# Configure OpenTelemetry exporter
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# Export to Application Insights
exporter = AzureMonitorTraceExporter(
    connection_string=os.getenv("APPLICATIONINSIGHTS_CONNECTION_STRING")
)

trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(exporter)
)

# Your LangGraph code
from langgraph.graph import StateGraph

graph = StateGraph(TravelPlanningState)
# ... build graph

# Traces automatically captured
result = await graph.ainvoke(state)
```

## What Gets Traced

### Graph-Level Traces

**Graph Execution:**
- Start/end of graph invocation
- Duration
- Input state
- Output state
- Success/failure

**Node Execution:**
- Each node invocation
- Node duration
- Input/output to each node
- Errors within nodes

**Edge Transitions:**
- Routing decisions
- Conditional edges
- State updates between nodes

### Agent-Level Traces

**LLM Calls:**
- Model name (gpt-4, gpt-3.5-turbo, etc.)
- Prompt text
- Completion text
- Token usage (prompt tokens, completion tokens)
- Latency
- Cost estimation

**Tool Invocations:**
- Tool name
- Input parameters
- Output/result
- Duration
- Success/failure

**Retrieval Operations** (if using RAG):
- Vector search queries
- Retrieved documents
- Relevance scores
- Search duration

## Generated Code Integration

GoalGen-generated projects automatically include AI Foundry tracing:

### In `langgraph/quest_builder.py`:

```python
"""
Quest Builder for {{ goal_id }}

Auto-generated by GoalGen with AI Foundry tracing
"""

from langgraph.graph import StateGraph
from typing import Dict, Any

# AI Foundry tracing (auto-enabled with langchain-azure-ai)
from langchain_azure_ai import init_tracing

# Initialize once at module load
init_tracing(
    project_name="{{ goal_id }}",
    export_to="application_insights"
)

def build_graph(goal_config: Dict[str, Any]) -> StateGraph:
    """
    Build LangGraph with automatic tracing

    All nodes, edges, and tool calls are automatically traced to AI Foundry
    """

    graph = StateGraph({{ goal_id | pascal_case }}State)

    # Add nodes (automatically traced)
    {% for agent_name, agent_config in agents.items() %}
    graph.add_node("{{ agent_name }}", {{ agent_name }}_node)
    {% endfor %}

    # Wire edges
    graph.set_entry_point("supervisor_agent")
    # ...

    return graph


# When graph runs, traces flow to Application Insights automatically
graph = build_graph(goal_config)
result = await graph.ainvoke(state, config={"configurable": {"thread_id": thread_id}})
```

### In `orchestrator/app/main.py`:

```python
"""
FastAPI Orchestrator with AI Foundry tracing
"""

from fastapi import FastAPI, Request
from langchain_azure_ai import init_tracing

# Initialize tracing for FastAPI app
init_tracing(
    project_name="{{ goal_id }}",
    export_to="application_insights"
)

app = FastAPI(title="{{ title }}")

@app.post("/message")
async def handle_message(request: Request):
    """
    Process user message

    Automatic tracing:
    - API request (method, path, duration, status)
    - LangGraph invocation
    - All agent/tool calls within graph
    - Response generation
    """

    thread_id = request.json().get("thread_id")
    message = request.json().get("message")

    # Graph invocation automatically traced end-to-end
    result = await graph.ainvoke(
        {"messages": [message]},
        config={"configurable": {"thread_id": thread_id}}
    )

    return {"response": result["messages"][-1].content}
```

## Viewing Traces in Azure AI Foundry

### 1. Navigate to AI Foundry Portal

```
https://ai.azure.com
→ Select your project (e.g., "travel-planning")
→ Go to "Observability" section
```

### 2. Trace Explorer

**View end-to-end traces:**
- Graph execution timeline
- Nested spans for each node
- LLM calls with prompts/completions
- Tool invocations
- Duration waterfalls

**Filter traces by:**
- Time range
- Success/failure
- Duration (find slow requests)
- Specific agents or tools
- User/thread ID

### 3. Performance Dashboards

**Pre-built dashboards show:**
- Requests per second
- Average latency
- P95, P99 latencies
- Error rates
- Token usage over time
- Cost per conversation
- Most used agents/tools

### 4. Kusto Queries (Application Insights)

**Query traces directly:**

```kusto
// Find slow graph executions
traces
| where name == "LangGraph.invoke"
| where duration > 5000  // > 5 seconds
| project timestamp, thread_id, duration, customDimensions
| order by duration desc

// Analyze tool usage
dependencies
| where type == "Tool"
| summarize count() by name, bin(timestamp, 1h)
| render timechart

// Track LLM token usage
customEvents
| where name == "LLMCall"
| extend prompt_tokens = toint(customDimensions.prompt_tokens)
| extend completion_tokens = toint(customDimensions.completion_tokens)
| summarize total_tokens = sum(prompt_tokens + completion_tokens) by bin(timestamp, 1h)
| render timechart

// Error analysis
exceptions
| where outerMessage contains "LangGraph"
| project timestamp, outerMessage, innerException
| order by timestamp desc
```

## Advanced Configuration

### Sampling

Control trace volume (for high-traffic apps):

```python
init_tracing(
    project_name="travel-planning",
    export_to="application_insights",
    sample_rate=0.1  # Trace 10% of requests
)
```

### Custom Attributes

Add custom attributes to traces:

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

async def flight_agent_node(state):
    with tracer.start_as_current_span("flight_search") as span:
        span.set_attribute("destination", state.get("destination"))
        span.set_attribute("budget", state.get("budget"))

        # Your agent logic
        result = await search_flights(state)

        span.set_attribute("flights_found", len(result))

        return result
```

### Correlation with User Sessions

```python
# In FastAPI handler
@app.post("/message")
async def handle_message(request: Request):
    user_id = request.headers.get("X-User-Id")
    thread_id = request.json().get("thread_id")

    # Add to trace context
    from opentelemetry import trace
    span = trace.get_current_span()
    span.set_attribute("user_id", user_id)
    span.set_attribute("thread_id", thread_id)

    # Graph execution traces will include these attributes
    result = await graph.ainvoke(...)
```

## Cost Tracking

AI Foundry automatically tracks LLM costs:

**Per-Request Cost:**
```kusto
customEvents
| where name == "LLMCall"
| extend model = tostring(customDimensions.model)
| extend prompt_tokens = toint(customDimensions.prompt_tokens)
| extend completion_tokens = toint(customDimensions.completion_tokens)
| extend cost = case(
    model == "gpt-4", (prompt_tokens * 0.03 + completion_tokens * 0.06) / 1000,
    model == "gpt-3.5-turbo", (prompt_tokens * 0.0005 + completion_tokens * 0.0015) / 1000,
    0.0
  )
| summarize total_cost = sum(cost) by bin(timestamp, 1d)
| render timechart
```

## Alerting

Set up alerts in Application Insights:

**High Latency Alert:**
```
Graph execution > 10 seconds
→ Send email to team
→ Create incident
```

**Error Rate Alert:**
```
Error rate > 5% over 5 minutes
→ Page on-call engineer
```

**Cost Alert:**
```
Daily LLM cost > $100
→ Send notification
```

## Debugging with Traces

**Scenario: "Graph is slow"**

1. Open Trace Explorer in AI Foundry
2. Filter by `duration > 5000ms`
3. Identify bottleneck node
4. Drill into that node's spans
5. See which tool call or LLM request is slow
6. Optimize that specific component

**Scenario: "Agent gives wrong answers"**

1. Find failing conversation in traces
2. View full LLM prompt sent
3. See what context was retrieved
4. Review tool outputs
5. Identify where logic went wrong
6. Fix prompt or tool

## Integration with GoalGen

**GoalGen automatically generates** AI Foundry-ready code:

✅ **langchain-azure-ai** in requirements.txt
✅ **init_tracing()** in quest_builder.py
✅ **Environment variables** in .env.sample
✅ **Bicep template** for Application Insights
✅ **Documentation** on viewing traces

**Generated `.env.sample`:**
```bash
# Azure AI Foundry
AZURE_AI_PROJECT_NAME=travel-planning
AZURE_SUBSCRIPTION_ID=xxx
AZURE_RESOURCE_GROUP=rg-ai-foundry

# Application Insights (auto-created by Bicep)
APPLICATIONINSIGHTS_CONNECTION_STRING=InstrumentationKey=xxx

# Tracing configuration
TRACING_ENABLED=true
TRACING_SAMPLE_RATE=1.0
```

## Best Practices

### 1. Enable Tracing in All Environments

```python
# Don't disable in production - you need it most there!
init_tracing(
    project_name="travel-planning",
    export_to="application_insights",
    enabled=os.getenv("TRACING_ENABLED", "true") == "true"
)
```

### 2. Use Sampling in High-Volume Apps

```python
# Production: 10% sampling
# Dev: 100% sampling
sample_rate = 0.1 if os.getenv("ENV") == "production" else 1.0

init_tracing(sample_rate=sample_rate)
```

### 3. Add Business Context

```python
span.set_attribute("conversation_type", "booking")
span.set_attribute("customer_tier", "premium")
span.set_attribute("region", "us-west")
```

### 4. Monitor Key Metrics

- **Latency**: P50, P95, P99 for graph execution
- **Error rate**: % of failed conversations
- **Token usage**: Track costs
- **Tool calls**: Which tools are used most

### 5. Set Up Alerts

- Graph execution > 10s
- Error rate > 5%
- Daily cost > threshold
- Specific tool failures

## Comparison with Manual Instrumentation

### Without AI Foundry (Manual):

```python
# You would need to manually instrument everything
import time
import logging

async def flight_agent_node(state):
    start = time.time()
    try:
        result = await search_flights(state)
        duration = time.time() - start
        logging.info(f"flight_search took {duration}s")
        return result
    except Exception as e:
        logging.error(f"flight_search failed: {e}")
        raise
```

### With AI Foundry (Automatic):

```python
# Zero instrumentation needed - just run your code
async def flight_agent_node(state):
    result = await search_flights(state)
    return result

# Tracing happens automatically via langchain-azure-ai
```

## Summary

**You are 100% correct!** Azure AI Foundry provides comprehensive, automatic observability for LangGraph:

✅ **Native Integration** - langchain-azure-ai package
✅ **OpenTelemetry Standard** - Industry-standard tracing
✅ **Zero Code Changes** - Automatic instrumentation
✅ **Full Visibility** - Graphs, agents, tools, LLMs
✅ **Production Ready** - Sampling, alerting, cost tracking
✅ **Unified Platform** - Works across all agent frameworks

**GoalGen generates AI Foundry-ready code** by default, so users get observability out of the box.

---

**References:**
- [Azure AI Foundry Tracing Docs](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/trace-agents-sdk)
- [LangChain Azure Integration](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/langchain)
- [Agent Tracing Integrations](https://learn.microsoft.com/en-us/azure/ai-foundry/observability/how-to/trace-agent-framework)

**Status**: Production-ready, actively maintained by Microsoft (2025)
