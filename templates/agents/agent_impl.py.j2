"""
Generated Agent: {{ agent_name }}
Goal: {{ goal_id }}

This file was generated by GoalGen.
It uses the GoalGen Core SDK (frmk) for Azure AI Foundry integration.
"""

from typing import Dict, Any
from langchain_core.messages import AIMessage

from frmk.agents.base_agent import BaseAgent


class {{ agent_name | pascal_case }}(BaseAgent):
    """
    {{ agent_name | replace('_', ' ') | title }}

    Kind: {{ agent.kind }}
    {% if agent.llm_config and agent.llm_config.model %}Model: {{ agent.llm_config.model }}
    {% endif %}Max Loops: {{ agent.max_loop | default(5) }}
    {% if agent.tools %}Tools: {{ agent.tools | join(', ') }}
    {% endif %}

    Capabilities:
    {% if agent.kind == 'supervisor' %}- Routes tasks to worker agents
    - Coordinates parallel execution
    - Aggregates results
    {% elif agent.kind == 'llm_agent' %}- Executes tasks using LLM reasoning
    - Uses tools to gather information
    - Provides detailed responses
    {% elif agent.kind == 'function_agent' %}- Executes predefined functions
    - No LLM reasoning
    - Fast, deterministic execution
    {% endif %}
    """

    def __init__(self, goal_config: Dict[str, Any]):
        """
        Initialize {{ agent_name }} agent

        The BaseAgent handles:
        - Loading prompts from Azure AI Foundry
        - Initializing LLM with config from spec
        - Binding tools from tool registry
        - Setting up logging and metrics
        """

        agent_config = goal_config["agents"]["{{ agent_name }}"]

        super().__init__(
            agent_name="{{ agent_name }}",
            agent_config=agent_config,
            goal_config=goal_config
        )

        # Agent-specific initialization
        self.logger.info("{{ agent_name }} initialized")

    async def _process_response(
        self,
        state: Dict[str, Any],
        response: AIMessage
    ) -> Dict[str, Any]:
        """
        Process LLM response and determine next action

        This is the agent-specific logic that varies by agent kind.
        """

        {% if agent.kind == 'supervisor' %}
        # Supervisor routing logic
        next_node = self._route_to_worker(state, response)

        return {
            "messages": state["messages"] + [response],
            "next": next_node,
            "supervisor_decision": response.content
        }

        {% elif agent.kind == 'llm_agent' %}
        # Standard LLM agent - check for tool calls
        if hasattr(response, 'tool_calls') and response.tool_calls:
            self.logger.info(f"Agent requesting {len(response.tool_calls)} tool calls")

            return {
                "messages": state["messages"] + [response],
                "next": "tools",  # Route to tool execution node
                "pending_tool_calls": response.tool_calls
            }

        # No tools needed - final response
        return {
            "messages": state["messages"] + [response],
            "next": "END"
        }

        {% elif agent.kind == 'function_agent' %}
        # Function agent - execute function directly
        result = await self._execute_function(state)

        return {
            "messages": state["messages"] + [
                AIMessage(content=str(result))
            ],
            "next": "END"
        }

        {% else %}
        # Default: return response and end
        return {
            "messages": state["messages"] + [response],
            "next": "END"
        }
        {% endif %}

    {% if agent.kind == 'supervisor' %}
    def _route_to_worker(
        self,
        state: Dict[str, Any],
        response: AIMessage
    ) -> str:
        """
        Supervisor routing logic

        Policy: {{ agent.policy }}
        Workers: {{ topology.get('workers', []) | join(', ') }}
        """

        {% if agent.policy == 'simple_router' %}
        # Simple keyword-based routing
        content = response.content.lower()

        {% for worker in topology.get('workers', []) %}
        # Check if response mentions {{ worker }}
        if "{{ worker }}" in content or "{{ worker.replace('_', ' ') }}" in content:
            self.logger.info(f"Routing to {{ worker }}")
            return "{{ worker }}"

        {% endfor %}

        # Check if all tasks complete
        if self._all_tasks_complete(state):
            return "END"

        # Continue coordinating
        return "supervisor"

        {% elif agent.policy == 'llm_router' %}
        # LLM-based routing - extract from response
        if "ROUTE:" in response.content:
            route = response.content.split("ROUTE:")[1].strip().split()[0]
            self.logger.info(f"LLM routed to: {route}")
            return route

        return "END"

        {% else %}
        # Custom routing policy
        # TODO: Implement custom routing logic
        return "END"
        {% endif %}

    def _all_tasks_complete(self, state: Dict[str, Any]) -> bool:
        """Check if all tasks are complete"""

        # Check if all required context fields are populated
        context = state.get("context", {})
        required_fields = {{ context_fields | tojson }}

        return all(context.get(field) is not None for field in required_fields)

    {% elif agent.kind == 'function_agent' %}
    async def _execute_function(self, state: Dict[str, Any]) -> Any:
        """Execute the agent's function"""

        # TODO: Implement function execution logic
        # This is where custom business logic goes

        return {"status": "executed", "agent": "{{ agent_name }}"}
    {% endif %}


# LangGraph node function
async def {{ agent_name }}_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    LangGraph node function for {{ agent_name }}

    This function is called by the LangGraph executor.
    It loads the goal config and invokes the agent.
    """

    # Load goal config (in production, this would be cached)
    import json
    import os

    config_path = os.path.join(
        os.path.dirname(__file__),
        "../../config/goal_spec.json"
    )

    with open(config_path) as f:
        goal_config = json.load(f)

    # Create and invoke agent
    agent = {{ agent_name | pascal_case }}(goal_config)
    return await agent.invoke(state)
