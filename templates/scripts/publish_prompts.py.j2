#!/usr/bin/env python3
"""
Publish Prompts to Azure AI Foundry
Generated by GoalGen

Uploads prompt templates to Azure AI Foundry Prompt Flow with versioning
"""

import os
import sys
import subprocess
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


def get_git_version():
    """Get git commit hash for versioning"""
    try:
        result = subprocess.run(
            ['git', 'rev-parse', '--short', 'HEAD'],
            capture_output=True,
            text=True,
            check=True
        )
        return result.stdout.strip()
    except subprocess.CalledProcessError:
        logger.warning("Git not available, using timestamp version")
        from datetime import datetime
        return datetime.now().strftime("%Y%m%d-%H%M%S")


def publish_prompt(prompt_file: Path, version: str, ai_foundry_project: str):
    """
    Publish a single prompt to Azure AI Foundry

    Args:
        prompt_file: Path to prompt markdown file
        version: Version string (git hash or timestamp)
        ai_foundry_project: AI Foundry project name
    """

    prompt_name = prompt_file.stem  # e.g., "supervisor_agent" from "supervisor_agent.md"

    logger.info(f"Publishing {prompt_name} (version: {version})...")

    try:
        # Read prompt content
        with open(prompt_file, 'r') as f:
            content = f.read()

        # TODO: Use Azure ML SDK to publish prompt
        # For now, this is a placeholder showing the intended API

        # from azure.ai.ml import MLClient
        # from azure.identity import DefaultAzureCredential
        # from azure.ai.ml.entities import Prompt

        # credential = DefaultAzureCredential()
        # ml_client = MLClient.from_config(credential=credential)

        # prompt = Prompt(
        #     name=prompt_name,
        #     version=version,
        #     content=content,
        #     tags={
        #         "goal_id": "{{ goal_id }}",
        #         "generated_by": "goalgen",
        #         "git_version": version
        #     }
        # )

        # ml_client.prompts.create_or_update(prompt)

        logger.info(f"✓ Published {prompt_name}:{version}")

        # For demonstration, just show what would be published
        logger.info(f"  Project: {ai_foundry_project}")
        logger.info(f"  Size: {len(content)} bytes")

        return True

    except Exception as e:
        logger.error(f"✗ Failed to publish {prompt_name}: {e}")
        return False


def main():
    """Main publishing workflow"""

    logger.info("=" * 60)
    logger.info("Publishing Prompts to Azure AI Foundry")
    logger.info("=" * 60)

    # Get configuration
    ai_foundry_project = os.getenv("AI_FOUNDRY_PROJECT", "{{ goal_id }}")
    ai_foundry_enabled = os.getenv("AI_FOUNDRY_ENABLED", "false").lower() == "true"

    if not ai_foundry_enabled:
        logger.warning("AI Foundry integration not enabled (AI_FOUNDRY_ENABLED=false)")
        logger.warning("Prompts will only be available locally in prompts/ directory")
        logger.info("To enable: Set AI_FOUNDRY_ENABLED=true")
        return 0

    # Get version
    version = os.getenv("PROMPT_VERSION") or get_git_version()
    logger.info(f"Version: {version}")
    logger.info(f"Project: {ai_foundry_project}")
    logger.info("")

    # Find all prompt files
    prompts_dir = Path(__file__).parent.parent / "prompts"

    if not prompts_dir.exists():
        logger.error(f"Prompts directory not found: {prompts_dir}")
        return 1

    prompt_files = list(prompts_dir.glob("*.md"))

    if not prompt_files:
        logger.warning("No prompt files found in prompts/")
        return 0

    logger.info(f"Found {len(prompt_files)} prompts to publish:")
    for pf in prompt_files:
        logger.info(f"  - {pf.name}")
    logger.info("")

    # Publish each prompt
    success_count = 0
    failed_count = 0

    for prompt_file in prompt_files:
        if publish_prompt(prompt_file, version, ai_foundry_project):
            success_count += 1
        else:
            failed_count += 1

    # Summary
    logger.info("")
    logger.info("=" * 60)
    logger.info(f"Published: {success_count}/{len(prompt_files)}")

    if failed_count > 0:
        logger.error(f"Failed: {failed_count}")
        return 1

    logger.info("✓ All prompts published successfully")
    return 0


if __name__ == "__main__":
    sys.exit(main())
